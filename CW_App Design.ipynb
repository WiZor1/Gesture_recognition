{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\python.exe\n",
      "C:\\Users\\WiZor\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n",
      "D:\\Programs\\Python\\Python38-32\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!where python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 21.3.1 from D:\\Programs\\Anaconda\\lib\\site-packages\\pip (python 3.8)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import imageio\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from model_define import SignDataset, MyResNet, df_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: docstrings...\n",
    "\n",
    "class GestureRecog():\n",
    "    def __init__(self,\n",
    "                 gesture_model: torch.nn.Module,\n",
    "                 face_detect_model: torch.nn.Module=None,\n",
    "                 transform: transforms.transforms.Compose=None,\n",
    "                 chain_delay: int=5,\n",
    "                 face_detect_freq: int=10,\n",
    "                 sign_dict: dict=None,\n",
    "                 sign_pics: dict=None,\n",
    "                 pic_size: tuple=(320, 240),\n",
    "                 text_color: tuple=(255, 255, 0),\n",
    "                 action_delay: int=3,\n",
    "                 save_imgs: bool=True,\n",
    "                 RESULTS_PATH: Path=Path('results'),\n",
    "                 ICONS_PATH: Path=Path('icons'),\n",
    "                 device='cpu'\n",
    "                ):\n",
    "        self._RESULTS_PATH = RESULTS_PATH\n",
    "        self._ICONS_PATH = ICONS_PATH\n",
    "        self._RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        self._ICONS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self._gesture_model = gesture_model\n",
    "        if face_detect_model is None:\n",
    "            self._face_detect_model = MTCNN(image_size=pic_size).to(device)\n",
    "        else:\n",
    "            self._face_detect_model = face_detect_model\n",
    "        if transform is None:\n",
    "            self._transform = df_transforms\n",
    "        else:\n",
    "            self._transform = transform\n",
    "        \n",
    "        self._res = None\n",
    "        self._chain_delay = chain_delay\n",
    "        self._face_detect_freq = face_detect_freq\n",
    "        \n",
    "        if sign_dict is None:\n",
    "            self._sign_dict = {0: 'Nothing',\n",
    "                               1: 'Minus',\n",
    "                               2: 'Greetings',\n",
    "                               3: 'Ok',\n",
    "                               4: 'Thumb up',\n",
    "                               5: 'Fist',\n",
    "                               6: 'Index',\n",
    "                               7: 'Two fingers'}\n",
    "        else:\n",
    "            self._sign_dict = sign_dict\n",
    "        if sign_pics is None:\n",
    "            self._sign_pics = {i: imageio.imread(self._ICONS_PATH / f'{self._sign_dict[i]}.png') for i in self._sign_dict.keys() if i != 0}\n",
    "        else:\n",
    "            self._sign_pics = sign_pics\n",
    "        \n",
    "        self.actions = {1: (self._default_1_act, 'fixed', {'text': 'Cancelled', 'display_time': 10, 'act_time': 0}),\n",
    "                        2: (self._default_2_act, 'fixed', {'text': 'Hello there!', 'display_time': 30, 'act_time': 0}),\n",
    "                        3: (self._default_3_act, 'fixed', {'text':  'Calc is opening...', 'display_time': 30, 'act_time': 30}),\n",
    "                        4: (self._default_4_act, 'fixed', {'text':'Session is ending...', 'display_time': 30, 'act_time': 30}),\n",
    "                        5: (self._default_5_act, 'continues', {'text': 'Icon is decreasing...', 'display_time': 1, 'act_time': 0}),\n",
    "                        6: (self._default_6_act, 'continues', {'text': 'Icon is increasing...', 'display_time': 1, 'act_time': 0}),\n",
    "                        7: (self._default_7_act, 'continues', {'text': 'Just test', 'display_time': 1, 'act_time': 0})}\n",
    "            \n",
    "        self._action_classification()\n",
    "        \n",
    "        \n",
    "        self._pic_size = pic_size\n",
    "        self._text_color = text_color\n",
    "        self._action_delay = action_delay\n",
    "        self._save_imgs = save_imgs\n",
    "        \n",
    "        self._resize_to = 100\n",
    "        self._face_detected = False\n",
    "        \n",
    "    def _action_classification(self):\n",
    "        self._cont_actions = [k for k, v in self.actions.items() if v[1] == 'continues']\n",
    "        self._fixed_actions = [k for k, v in self.actions.items() if v[1] == 'fixed']\n",
    "    \n",
    "    def _query_fill(self, action):\n",
    "        act_dict = self.actions[action][2]\n",
    "        self._query.append([self.actions[action][0], act_dict['text'], act_dict['display_time'], act_dict['act_time']])\n",
    "        \n",
    "    def _query_clear(self, first_elem=None):\n",
    "        if first_elem is None:\n",
    "            self._query = []\n",
    "        else:\n",
    "            self._query = [first_elem]\n",
    "    \n",
    "    def _query_update(self):\n",
    "        q = self._query.copy()\n",
    "        for i in range(len(q) - 1, -1, -1):\n",
    "            if q[i][3] == 0:\n",
    "                self._do_now.append(q[i][0])\n",
    "            if q[i][2] > 0:\n",
    "                self._query[i][2] -= 1\n",
    "                self._query[i][3] -= 1\n",
    "            else:\n",
    "                self._query.pop(i)\n",
    "    \n",
    "    def _do_actual(self):\n",
    "        for elem in self._do_now:\n",
    "            elem()\n",
    "        self._do_now = []\n",
    "    \n",
    "    def _fake_act(self):\n",
    "        pass\n",
    "    \n",
    "    def _default_1_act(self):\n",
    "        self._query_clear([self._fake_act] + list(self.actions[1][2].values()))\n",
    "    \n",
    "    def _default_2_act(self):\n",
    "        pass\n",
    "    \n",
    "    def _default_3_act(self):\n",
    "        file_name = 'calc'\n",
    "        os.system(file_name + '.exe')\n",
    "    \n",
    "    def _default_4_act(self):\n",
    "        self._end_time = dt.datetime.now()\n",
    "    \n",
    "    def _default_5_act(self):\n",
    "        self._resize_to -= 10\n",
    "        self._resize_to = max(10, self._resize_to)\n",
    "    \n",
    "    def _default_6_act(self):\n",
    "        self._resize_to += 10\n",
    "        self._resize_to = min(300, self._resize_to)\n",
    "    \n",
    "    def _default_7_act(self):\n",
    "        pass\n",
    "    \n",
    "    def _init_run(self):\n",
    "        self._delete_imgs()\n",
    "        self._end_time = None\n",
    "        self._iteration = 0\n",
    "        self._last_shown = 0\n",
    "        self._query = []\n",
    "        self._do_now = []\n",
    "        self._started_at = dt.datetime.now()\n",
    "        \n",
    "    def _chain_res_builder(self, res):\n",
    "        if self._res is None:\n",
    "            self._res = res.clone().detach()\n",
    "        else:\n",
    "            self._res = torch.concat((self._res, res), dim=0)[-self._chain_delay:].clone().detach()\n",
    "    \n",
    "    def _res_calc(self):\n",
    "        res = self._res.mean(dim=0)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def _gesture_detect(self,\n",
    "                        img: np.array,\n",
    "                        transform: transforms.transforms.Compose,\n",
    "                        model: torch.nn.Module):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "        img = transform(img)\n",
    "        res = F.softmax(model(img[None]), dim=1)\n",
    "        self._chain_res_builder(res)\n",
    "        res_final = self._res_calc()\n",
    "        self.shown = res_final.argmax(dim=0).detach().numpy().tolist()\n",
    "        self.shown_perc = res_final.max(dim=0).values.detach().numpy().tolist()\n",
    "        \n",
    "        return res_final, img\n",
    "\n",
    "    def _gesture_pic_draw(self, frame, pos):\n",
    "        x_offset, y_offset = pos\n",
    "        small_img = cv2.resize(self._sign_pics[self.shown], (self._resize_to, self._resize_to))\n",
    "        large_img = frame.copy()\n",
    "\n",
    "        small_img[:, :, 0:3] = self._text_color\n",
    "\n",
    "        rows, columns, chanels = small_img[:, :, :3].shape\n",
    "        roi = large_img[y_offset:self._resize_to+y_offset,\n",
    "                        x_offset:self._resize_to+x_offset]\n",
    "        mask = 255 - small_img[:, :, 3]\n",
    "        bg = cv2.bitwise_or(roi, roi, mask = mask)\n",
    "        mask_inv = 255 - mask\n",
    "        fg = cv2.bitwise_and(small_img[:, :, :3], small_img[:, :, :3], mask=mask_inv)\n",
    "        final_roi = cv2.add(bg, fg)\n",
    "        small_img = final_roi\n",
    "        frame[y_offset:y_offset+small_img.shape[0],\n",
    "              x_offset:x_offset+small_img.shape[1]] = small_img\n",
    "        \n",
    "        return frame\n",
    "        \n",
    "    def _put_text_shown(self, frame, pos):\n",
    "        frame = frame.copy()\n",
    "        if self._face_detected:\n",
    "            text = f'{self._sign_dict[self.shown]} shown [{round(self.shown_perc * 100, 2)}%]'\n",
    "        else:\n",
    "            text = 'Face not found'\n",
    "        cv2.putText(frame,\n",
    "                    text,\n",
    "                    pos,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    (0, 0, 0),\n",
    "                    2,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.putText(frame,\n",
    "                    text,\n",
    "                    pos,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    self._text_color,\n",
    "                    1,\n",
    "                    cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _put_text_activity(self, frame, activity, pos):\n",
    "        frame = frame.copy()\n",
    "        cv2.putText(frame, activity, pos, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, activity, pos, cv2.FONT_HERSHEY_SIMPLEX, 0.8, self._text_color, 1, cv2.LINE_AA)\n",
    "    \n",
    "        return frame\n",
    "    \n",
    "    def _put_text_fps(self, frame, fps, pos):\n",
    "        frame = frame.copy()\n",
    "        cv2.putText(frame, f'FPS={round(fps, 1)}', pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f'FPS={round(fps, 1)}', pos, cv2.FONT_HERSHEY_SIMPLEX, 0.5, self._text_color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def _delete_imgs(self):\n",
    "        [f.unlink() for f in self._RESULTS_PATH.glob('*.png')]\n",
    "    \n",
    "    def _face_detect(self, frame):\n",
    "        face_detect = self._face_detect_model.detect(frame, landmarks=False)\n",
    "        self._face_detected = False if face_detect[0] is None else True\n",
    "    \n",
    "    def _make_gif(self, duration):\n",
    "        files = list(self._RESULTS_PATH.glob('*.png'))\n",
    "\n",
    "        with imageio.get_writer(self._RESULTS_PATH /'sign_vis.gif', mode='I', duration=duration / len(files)) as writer:\n",
    "            for file in sorted([f for f in files], key=lambda x: x.stat().st_ctime):\n",
    "                image = imageio.imread(file)\n",
    "                writer.append_data(image)\n",
    "        self._delete_imgs()\n",
    "    \n",
    "    def run(self):\n",
    "        self._init_run()\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        try:\n",
    "            while True:\n",
    "                start = dt.datetime.now()\n",
    "                _, frame = cap.read()\n",
    "                frame = cv2.resize(frame, self._pic_size)\n",
    "                if not self._iteration%self._face_detect_freq:\n",
    "                    self._face_detect(frame)\n",
    "                    \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "                if self._face_detected:\n",
    "                    res, img = self._gesture_detect(frame, self._transform, self._gesture_model)\n",
    "                    \n",
    "                    if self.shown != 0:\n",
    "                        if self.shown in self._cont_actions:\n",
    "                            self._query_fill(self.shown)\n",
    "                        elif self.shown in self._fixed_actions and self.shown != self._last_shown:\n",
    "                            self._query_fill(self.shown)\n",
    "                            \n",
    "                        frame = self._gesture_pic_draw(frame, (20, 70 + 20 * len(self._query)))\n",
    "                    \n",
    "                    self._last_shown = self.shown\n",
    "                    \n",
    "                self._query_update()\n",
    "                        \n",
    "                for i, item in enumerate(self._query):\n",
    "                    frame = self._put_text_activity(frame, item[1], pos=(20, 60 + 20 * i))\n",
    "\n",
    "                frame = self._put_text_shown(frame, pos=(20, 30))\n",
    "\n",
    "                self._do_actual()\n",
    "\n",
    "                if self._end_time is not None and self._end_time < dt.datetime.now():\n",
    "                    break\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                try:\n",
    "                    fps = 1 / (dt.datetime.now() - start).total_seconds()\n",
    "                except ZeroDivisionError:\n",
    "                    fps = 100500\n",
    "                print(f'FPS = {fps}')\n",
    "                frame = self._put_text_fps(frame, fps, pos=(10, 470))\n",
    "                \n",
    "                cv2.imshow('Normal Video', frame)\n",
    "                \n",
    "                if self._save_imgs:\n",
    "                    cv2.imwrite(str(self._RESULTS_PATH / f'{self._iteration}.png'), frame)\n",
    "                self._iteration += 1\n",
    "\n",
    "        finally:\n",
    "            duration = (dt.datetime.now() - self._started_at).total_seconds()\n",
    "            cv2.destroyAllWindows()\n",
    "            cap.release()\n",
    "            if self._save_imgs:\n",
    "                self._make_gif(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_o = GestureRecog(torch.load('models/sign_d_ep_15_acc_0.998', map_location=torch.device(device)),\n",
    "                     pic_size=(640, 480),\n",
    "                     save_imgs=False,\n",
    "                     face_detect_freq=10,\n",
    "                     chain_delay=10,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS = 12.345831430017654\n"
     ]
    }
   ],
   "source": [
    "new_o.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "# Общее описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая стадия - определение наличия лица на входящем видео потоке. Если лицо найдено, идет определение жеста на ряде следующих кадров. Далее в зависимости от жеста выполняется конкретное действие, все они будут описаны ниже.\n",
    "\n",
    "***Крайне рекомендуется запускать из локального `Jupyter Notebook` из-за использования модуля `cv2.imshow`, напрямую не поддерживаемого в Colab***\n",
    "\n",
    "**Особенности реализации**:\n",
    "* за модель распознавания жестов из коробки взят аналог `ResNet18`;\n",
    "* при использовании стандартной `MTCNN` и модели распознавания жеста из коробки при угадывании жеста без оптимизаций количество кадров в секунду приблизительно равно 5, если же распознавать лицо 1 раз в 10 кадров (как настроено сейчас), выстродействие вырастает более чем в 2 раза - в среднем 12 кадров в секунду на моем железе (дальнейшую оптимизацию можно делать, изменяя архитектуру модели распознавания жестов). Без фиксации лица модель может работать со скорость в ~120 кадров в секунду, но камера у меня в 30 ¯\\\\\\_(ツ)\\_/¯ ;\n",
    "* распознавание жеста происходит не по 1 кадру, а по серии (настраиваемый параметр, по умолчанию 10), в рамках которой идет усреднение результата и выбора максимального. Такой подход позволяет избегать случайного фиксирования какого-либо жеста при перестроении руки (например, после показанного пальца вверх при убирании руки не будет фикироваться кулак, который скорее всего будет на мгновение показан);\n",
    "* \n",
    "* сейчас стандартная модель обучена только на датасете моих жестов и в ограниченном пространстве, поэтому может иметь место некоторое переобучение, в дальнейшем требуется расширение датасета и дообучение модели распознавания жестов.\n",
    "\n",
    "**Интерфейс и что вообще происходит**:\n",
    "* в самом левом верхнем углу показано название жеста и уверенность модели;\n",
    "* на жесты, требующие какого-либо действия, ниже названия жеста должна появиться строка с выполняемым действием (на текущий момент настроена задержка в 30 кадров для того, чтобы успеть отменить жест при ошибочной регистрации жеста);\n",
    "* после показанного жеста модель должна реагировать иконкой такого же жеста в левом верхнем углу (при отсутствии жеста ничего не показывается);\n",
    "* есть 2 типа активностей при жестах: *продолжительные* (нужно держать руку для постоянного выполнения действия) и *фиксированные* (после первого показа жеста начинается выполнение действия и дальнейшие эти же жесты игнорируются до смены жеста на другой). Обычно текст для продолжительных указывается во время выполнения самого действия, для фиксированного текст пишется до начала момента выполнения действия (во время вывода текста программа просто ждет);\n",
    "* если были показаны подряд несколько жестов, программа продолжит корректно работать и запишет все действия в очередь, из которой они потом поступательно будут выполняться;\n",
    "* предусмотрена отмена всех действия, попавших в очередь (жест по умолчанию - показанная боком правая рука ладонью вниз);\n",
    "* можно сохранить видео ряд в формате `gif` установив `save_imgs` при объявлении объекта в `True`. Результат будет сохранен в `RESULTS_PATH` (по умолчанию `./results`, которая будет создана при ее отсутствии)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/face_off.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Возможности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Приветствие (*ладонь*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель умеет отвечать на приветственный жест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/greetings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Увеличение иконки жеста (*палец вверх*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию увеличение на 10 пикселей за 1 кадр до максимума в 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/upscale.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Уменьшение иконки жеста (*кулак*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию уменьшение на 10 пикселей за 1 кадр до минимума в 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/downscale.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Открытие внешней программы (*ОК*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию открывается калькулятор. На гиф этого, к сожалению, не видно, но присутствует описание действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/calc.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Отмена действия (*ладонь вниз, рука повернута боком*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства действие `Cancelled` демонстрируется еще 10 кадров (~ 1 секунду) после отмены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/cancel.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Завершение работы программы (*палец вверх*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В примерах ранее закрытие происходило вручную. Здесь демонстрируется закрытие жестом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/end.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комплексная демонстрация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже когда все реализовал, пожалел, что не добавил какой-либо разделитель в начало или конец гиф, а то не совсем понятно, где они. Цикличность ¯\\\\\\_(ツ)\\_/¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](results/main.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
